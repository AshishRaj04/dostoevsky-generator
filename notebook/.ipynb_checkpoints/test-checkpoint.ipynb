{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e593bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5121020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f9f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b38a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import create_transformer , compute_loss\n",
    "from tokenization import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ef5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280dc326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value: 3.9200823307037354\n"
     ]
    }
   ],
   "source": [
    "# xb = tf.random.uniform((4, 8), maxval=26, dtype=tf.int32)\n",
    "# targets = tf.random.uniform((4, 8), maxval=26, dtype=tf.int32)\n",
    "# loss = compute_loss(model, xb, targets)\n",
    "# loss_value = loss.numpy()\n",
    "# print(f\"Loss value: {loss_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9cef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(),'..','data')\n",
    "sonnet_path = os.path.join(data_dir , 'sonnets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ed31c7-828e-48ef-ba5a-d80a45ed85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sonnet_path , \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf827cd-3146-41a2-9b6e-fd11b5d834a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(text, vocab_size=512 , verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "309fca2f-aff8-490f-9692-5ff7bcc453e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(text)\n",
    "# print(\"Encoded:\", encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f27111ef-676a-4ed7-ae1e-4df8f31999a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded token length: 43787\n",
      "text characters length: 93578\n",
      "compression ratio: 2.14X\n"
     ]
    }
   ],
   "source": [
    "print(\"encoded token length:\", len(encoded))\n",
    "print(\"text characters length:\", len(text))\n",
    "print(f\"compression ratio: {len(text) / len(encoded):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd6f601a-e1bd-4335-9b81-a6f0ef462972",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(encoded)\n",
    "# print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef74af-0bbd-45d4-b8e4-5f6db6fe4a81",
   "metadata": {},
   "source": [
    "### Create sequences using `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0abdcb7e-4afc-42f2-b70d-36ae19c01a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "buffer_size = 10000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b974377-3807-4f17-8a7d-1e5ed8ad24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ds = tf.data.Dataset.from_tensor_slices(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60719048-00b4-4d05-86b0-6993d42ae774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences of seq_length+1 (last one is the target)\n",
    "sequences = token_ds.window(size=context_length+1, shift=1, drop_remainder=True)\n",
    "sequences = sequences.flat_map(lambda window: window.batch(context_length + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58d68105-6a4b-4687-96fe-ee41b7b053e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(seq):\n",
    "    input_text = seq[:-1]\n",
    "    target_text = seq[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "464f1a97-493f-44c7-bebb-3a59403fba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb417002-c6a0-495e-b407-be74c0d826a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3acbc94-5f41-45a2-a6ff-91386f7afc25",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DecoderTransformer.__init__() missing 4 required positional arguments: 'vocab_size', 'emb_dim', 'n_heads', and 'Nx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Transformer\\src\\transformer.py:140\u001b[0m, in \u001b[0;36mcreate_transformer\u001b[1;34m()\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_transformer\u001b[39m(vocab_size , emb_dim , n_heads ,  Nx):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDecoderTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: DecoderTransformer.__init__() missing 4 required positional arguments: 'vocab_size', 'emb_dim', 'n_heads', and 'Nx'"
     ]
    }
   ],
   "source": [
    "model = create_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734a32b-5639-42c8-a78e-e5675f0adc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cde22fc6-b9dc-4ca5-9079-e5021c389196",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca21a967-e3e6-451f-901a-6847251e8771",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m----> 6\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompute_loss(xb, yb)\n\u001b[0;32m      8\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    losses = []\n",
    "\n",
    "    for xb, yb in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = model.compute_loss(xb, yb)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "    print(f\"epoch={epoch}, loss={loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e4193-b9ed-4923-b8eb-9dc3d4d4bde4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656d99e-c091-4759-bf32-8c36dc60a482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea283b-1a24-4693-8584-ccd59759d7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40d0c0-e061-410f-ba68-3fd3a04dcbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc1f27-b324-401c-b504-f645ed62f8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
